this is still empty repository, I am planning to add the code from Colab within a few days

# Transformer_From_Scratch
The code (from scratch, Python, MXNET) for Transformer

I use d2l.ai project http://d2l.ai/ to implement code for the Transformer (machine translation) from scratch. Original paper for the Transformer architecture: 
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems (pp. 5998–6008).  
It is not easy to understand how the Transformer works. I added a lot of comments at most difficult 
parts of the code and many code examples. With it, I hope, it will be easier to learn the architecture. 

As an example the Transformer is used for English French translation on 
small amount of data (about 100000 pairs of sentences). 

The Transformer is used for experiments with different parameters. 

It is the first part of my project in Transformers. The next steps: 

1.Translation on English Finnish pairs dataset.   
2.Translation on big datasets.  
3.Modifications of the original Transformer (change in architecture).  
4.Development of the theory of the Transformers.  
5.Transformers from scratch in Rust.   
6.I am planing to add some explanation of the architecture using the ket / bra 
notations. 

